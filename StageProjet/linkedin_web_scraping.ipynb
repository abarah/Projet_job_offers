{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fdf615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def main_linkedin(keyword, location, InternType):\n",
    "    # Initialize the webdriver\n",
    "    driver = webdriver.Chrome(executable_path='C:\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "\n",
    "    if  InternType == \"PFE\":\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_E=2&f_TP=1&keywords={keyword}&location={location}'\n",
    "    elif InternType == \"PFA\":\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_E=2&f_TP=1&f_JT=2&keywords={keyword}&location={location}'\n",
    "    else :\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_JT=F&keywords=stage%20{keyword}&location={location}'\n",
    "    # Navigate to the LinkedIn job search page\n",
    "   # url = f'https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}'\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Initialize a counter for the number of pages scraped\n",
    "    page_count = 1\n",
    "\n",
    "    # Loop through the first 4 pages of results\n",
    "    while page_count <= 4:\n",
    "\n",
    "        # Get the HTML of the current page\n",
    "        page_html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "        # Find all the job listings on the page\n",
    "        jobs_list = soup.find_all('div', {'class': 'job-search-card'})\n",
    "\n",
    "        # Loop through the job listings and extract the relevant data\n",
    "        for job in jobs_list:\n",
    "            title = job.h3.text.strip()\n",
    "            company = job.h4.text.strip()\n",
    "            location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "            link = job.a.get('href')\n",
    "\n",
    "            # Open the job page to get the full job description\n",
    "            driver.get(link)\n",
    "            time.sleep(5)\n",
    "            job_html = driver.page_source\n",
    "            job_soup = BeautifulSoup(job_html, 'html.parser')\n",
    "            description = job_soup.find('div', {'class': 'description__text'}).text.strip()\n",
    "\n",
    "            # Write the data to a CSV file\n",
    "            with open('linkedin_data.csv', mode='a', encoding='utf-8', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Title', 'Company', 'Location', 'Description', 'Link'])\n",
    "\n",
    "            # Print the job data to the console\n",
    "            print(f'{title} ({company}) - {location}\\nLink: {link}\\nDescription: {description}\\n')\n",
    "\n",
    "        # Click the \"Next\" button to go to the next page\n",
    "        try:\n",
    "            next_button = driver.find_element_by_xpath('//button[@aria-label=\"Next\"]')\n",
    "            next_button.click()\n",
    "            page_count += 1\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print('No more pages.')\n",
    "            break\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2bb248",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def main_linkedin(keyword, location, InternType):\n",
    "    # Initialize the webdriver\n",
    "    driver = webdriver.Chrome(executable_path='C:\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe')\n",
    "\n",
    "\n",
    "    if  InternType == \"PFE\":\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_E=2&f_TP=1&keywords={keyword}&location={location}'\n",
    "    elif InternType == \"PFA\":\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_E=2&f_TP=1&f_JT=2&keywords={keyword}&location={location}'\n",
    "    else :\n",
    "        url = f'https://ma.linkedin.com/jobs/search/?f_JT=F&keywords=stage%20{keyword}&location={location}'\n",
    "    # Navigate to the LinkedIn job search page\n",
    "   # url = f'https://www.linkedin.com/jobs/search/?keywords={keyword}&location={location}'\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the page to load\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Initialize a counter for the number of pages scraped\n",
    "    page_count = 1\n",
    "\n",
    "    # Loop through the first 4 pages of results\n",
    "    while page_count <= 4:\n",
    "\n",
    "        # Get the HTML of the current page\n",
    "        page_html = driver.page_source\n",
    "\n",
    "        # Parse the HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "        # Find all the job listings on the page\n",
    "        jobs_list = soup.find_all('div', {'class': 'job-search-card'})\n",
    "\n",
    "        # Loop through the job listings and extract the relevant data\n",
    "        for job in jobs_list:\n",
    "            title = job.h3.text.strip()\n",
    "            company = job.h4.text.strip()\n",
    "            location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "            link = job.a.get('href')\n",
    "\n",
    "            # Open the job page to get the full job description\n",
    "            driver.get(link)\n",
    "            time.sleep(5)\n",
    "            job_html = driver.page_source\n",
    "            job_soup = BeautifulSoup(job_html, 'html.parser')\n",
    "            description = job_soup.find('div', {'class': 'description__text'}).text.strip()\n",
    "\n",
    "            # Write the data to a CSV file\n",
    "            with open('linkedin_data.csv', mode='a', encoding='utf-8', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(['Title', 'Company', 'Location', 'Description', 'Link'])\n",
    "\n",
    "            # Print the job data to the console\n",
    "            print(f'{title} ({company}) - {location}\\nLink: {link}\\nDescription: {description}\\n')\n",
    "\n",
    "        # Click the \"Next\" button to go to the next page\n",
    "        try:\n",
    "            next_button = driver.find_element_by_xpath('//button[@aria-label=\"Next\"]')\n",
    "            next_button.click()\n",
    "            page_count += 1\n",
    "            time.sleep(5)\n",
    "        except:\n",
    "            print('No more pages.')\n",
    "            break\n",
    "\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93121291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
